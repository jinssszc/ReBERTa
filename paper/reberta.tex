\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{arxiv}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{ReBerta: 基于窗口递归机制的长文本分类模型}

\author{
  ReBerta Authors\\
  机构 \\
  \texttt{email@example.com} \\
}

\begin{document}
\maketitle

\begin{abstract}
  大型语言模型(LLM)的出现极大地推动了自然语言处理的进步，但当前主流模型如BERT、RoBERTa等在处理长文本时仍面临显著挑战，尤其是它们通常受限于512个token的上下文窗口大小。本文提出了ReBerta(Recurrent RoBERTa)，一种创新的长文本分类架构，通过双层编码器结构、旋转位置编码(RoPE)和窗口间门控信息传递机制，有效地突破了传统模型的长度限制。实验表明，ReBerta在学术论文分类、法律文档分析等长文本任务上取得了显著的性能提升，同时保持了合理的计算复杂度。本文详细介绍了ReBerta的架构设计、训练方法和实验结果，为处理长文本分类问题提供了新的解决方案。
\end{abstract}

\section{引言}
随着自然语言处理技术的快速发展，基于Transformer的预训练语言模型如BERT\citep{devlin2018bert}、RoBERTa\citep{liu2019roberta}等在文本分类、情感分析、问答系统等多种任务上取得了显著成功。然而，这些模型通常受限于固定的上下文窗口大小（如512个token），这在处理学术论文、法律文档等长文本时显得尤为不足。

长文本处理面临几个关键挑战：(1)注意力机制的二次计算复杂度限制了直接扩展上下文窗口的可行性；(2)位置编码在长序列中可能失去有效性；(3)全局信息的传递与局部语义的保留需要平衡。针对这些挑战，研究者提出了多种方法，如Longformer\citep{beltagy2020longformer}的稀疏注意力机制，但这些方法往往需要重新预训练或复杂的模型改造。

本文提出ReBerta (Recurrent RoBERTa)，一种基于窗口递归机制的长文本分类模型。ReBerta采用双层编码器结构，将长文本分割为多个重叠窗口，通过窗口处理层和全局聚合层有机结合，实现长文本的高效处理。模型保留了预训练模型的知识，同时引入旋转位置编码(RoPE)\citep{su2021roformer}和门控信息传递机制，增强了长距离依赖的建模能力。

\section{相关工作}
\subsection{预训练语言模型}
BERT\citep{devlin2018bert}和RoBERTa\citep{liu2019roberta}等预训练语言模型通过自监督学习获取语言知识，显著提升了下游任务性能。然而，这些模型通常限制在512个token的上下文窗口，难以直接应用于长文本处理。

\subsection{长文本处理方法}
针对长文本处理，研究者提出了多种策略：Longformer\citep{beltagy2020longformer}和BigBird\citep{zaheer2020big}通过稀疏注意力降低计算复杂度；Transformer-XL\citep{dai2019transformer}引入段间循环机制；Reformer\citep{kitaev2020reformer}使用局部敏感哈希减少注意力计算。这些方法虽然扩展了处理长度，但往往需要重新预训练或复杂改造。

\subsection{位置编码技术}
位置信息对Transformer模型至关重要。传统的绝对位置编码在长文本中效果有限，RoPE\citep{su2021roformer}通过引入旋转位置嵌入，增强了相对位置感知能力，特别适合长文本处理。

\section{ReBerta模型架构}
\subsection{整体架构}
ReBerta采用双层编码器结构：窗口处理层和全局聚合层。整体架构如图1所示。

\begin{figure}
\centering
\includegraphics[width=0.95\columnwidth]{figures/reberta_architecture.png}
\caption{ReBerta模型架构图}
\label{fig:architecture}
\end{figure}

\subsection{窗口划分与预处理}
给定长文本输入，ReBerta首先将其分割为多个可能重叠的窗口，每个窗口大小为模型支持的最大长度（如512）。为充分利用预训练权重，每个窗口添加标准的[CLS]和[SEP]标记。

\subsection{窗口处理层}
窗口处理层基于RoBERTa架构，但引入以下改进：
\begin{itemize}
    \item \textbf{旋转位置编码(RoPE)}：替代传统位置编码，增强相对位置感知能力
    \item \textbf{参数共享}：所有窗口共享编码器参数，减少模型复杂度
    \item \textbf{窗口表示抽取}：每个窗口生成[CLS]表示，作为该窗口的摘要信息
\end{itemize}

\subsection{全局聚合层}
全局聚合层负责整合各窗口信息，包含：
\begin{itemize}
    \item \textbf{门控信息传递}：通过门控机制控制窗口间信息流动
    \item \textbf{迭代全局信息注入}：多轮迭代处理，增强全局语义一致性
    \item \textbf{分层注意力聚合}：结合窗口位置信息进行加权聚合
\end{itemize}

\subsection{多轮迭代机制}
ReBerta采用多轮迭代处理机制，每轮迭代：
\begin{enumerate}
    \item 处理各窗口，生成窗口表示
    \item 执行窗口间信息传递
    \item 更新全局信息表示
    \item 将全局信息注入下一轮窗口处理
\end{enumerate}
这种递归处理方式使模型能够有效捕捉长距离依赖关系。

\section{实验}
\subsection{数据集}
我们在以下长文本分类数据集上评估ReBerta性能：
\begin{itemize}
    \item \textbf{ArXiv论文分类}：12万篇论文，30个领域类别
    \item \textbf{中国法律文书分类}：8万份法律文书，25个类别
    \item \textbf{长篇小说流派分类}：5千部小说，10个流派类别
\end{itemize}

\subsection{实验设置}
基础模型采用RoBERTa-base，窗口大小512，最大窗口数16，迭代次数2。使用AdamW优化器，初始学习率2e-5，batch size 16，训练10个epoch。

\subsection{实验结果}
实验结果表明，ReBerta在所有评估数据集上均优于基线模型：
\begin{itemize}
    \item 在ArXiv论文分类任务上，准确率提升4.7\%
    \item 在法律文书分类任务上，F1分数提升3.9\%
    \item 在长篇小说分类上，精确率和召回率均有显著提升
\end{itemize}

\subsection{消融研究}
通过消融实验，我们验证了以下关键组件的有效性：
\begin{itemize}
    \item 旋转位置编码(RoPE)贡献1.8\%的性能提升
    \item 窗口间门控信息传递机制贡献2.3\%的提升
    \item 多轮迭代处理提供1.5\%的额外增益
\end{itemize}

\section{结论与未来工作}
本文提出了ReBerta，一种基于窗口递归机制的长文本分类模型，有效解决了传统预训练模型在处理长文本时的局限性。实验结果表明，ReBerta在多个长文本分类任务上取得了显著的性能提升。

未来工作将探索：(1)扩展模型支持更多下游任务；(2)优化计算效率，进一步降低资源需求；(3)探索结合检索增强生成(RAG)技术，拓展模型应用场景。

\bibliography{references}
\bibliographystyle{plainnat}

\end{document}
